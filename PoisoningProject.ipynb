{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P8mrwYaGlVj"
      },
      "source": [
        "# **Defending Against Poisoned Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHZacRNMGN2i"
      },
      "source": [
        "This project aims to build a simple image classifier and poison a small subset of the data it is trained on to misclassify a specific target image. Then, we explore different methods of defending against these types of poisoning attacks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: We saved our models after training so that we didn't have to retrain them each time we trained and tested a new model. For anyone running our notebook, you can either just ignore those cells and re-train each model (about 10 mins each), or we will include the pretrained models in our sumbission and you can ignore the training cells and just load in the pre trained models and then run the evaluation cells."
      ],
      "metadata": {
        "id": "rXPi447TJO_U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnk2dCb2bzTJ"
      },
      "source": [
        "# Basic Image Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkrmP549bh7j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# CIFAR-10 normalization constants\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# Data augmentation for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "# No augmentation for testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", train_set.classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uopSdkFNb0WH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# ResNet18 - modify for CIFAR-10 (32x32 images)\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "print(f\"Model ready on: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux4oF6g5b0_X"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)    # small lr is safer for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f49eIyusb6Iy"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn2csLUqb7r6"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}  |  \"\n",
        "          f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONz7DpWnMiwg"
      },
      "outputs": [],
      "source": [
        "### FOR LOADING THE SAVED MODEL (use if loading a previously trained model)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS260D_Final_Project/model_baseline.pth\"))\n",
        "test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EK9kZKqcF9x"
      },
      "source": [
        "# Poisoning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "190c5e10"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "target_image = None\n",
        "target_true_label = None\n",
        "target_predicted_label = None\n",
        "target_probabilities = None\n",
        "max_dog_prob = -1.0\n",
        "\n",
        "# Get class indices\n",
        "deer_idx = train_set.classes.index('deer')\n",
        "dog_idx = train_set.classes.index('dog')\n",
        "\n",
        "print(f\"Searching for a 'deer' image with high 'dog' probability...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            true_label = labels[i].item()\n",
        "            predicted_label = predicted[i].item()\n",
        "\n",
        "            # Check if it's a 'deer' and correctly classified as 'deer'\n",
        "            if true_label == deer_idx and predicted_label == deer_idx:\n",
        "                current_dog_prob = probabilities[i, dog_idx].item()\n",
        "\n",
        "                if current_dog_prob > max_dog_prob:\n",
        "                    max_dog_prob = current_dog_prob\n",
        "                    target_image = images[i].cpu()\n",
        "                    target_true_label = true_label\n",
        "                    target_predicted_label = predicted_label\n",
        "                    target_probabilities = probabilities[i].cpu()\n",
        "\n",
        "# Print the results for the selected target image\n",
        "if target_image is not None:\n",
        "    print(f\"\\n--- Selected Target Image Details ---\")\n",
        "    print(f\"True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "    print(f\"Predicted Class: {train_set.classes[target_predicted_label]} (Index: {target_predicted_label})\")\n",
        "\n",
        "    # Get top 5 probabilities and classes\n",
        "    top5_probs, top5_indices = torch.topk(target_probabilities, 5)\n",
        "    print(\"Top 5 Predicted Probabilities and Classes:\")\n",
        "    for i in range(5):\n",
        "        class_name = train_set.classes[top5_indices[i].item()]\n",
        "        probability = top5_probs[i].item()\n",
        "        print(f\"  {class_name}: {probability:.4f}\")\n",
        "    print(f\"\\nThis 'deer' image was chosen because it is correctly classified, but the model assigns a notably high probability to the 'dog' class ({max_dog_prob:.4f}), making it a suitable target for a data poisoning attack.\")\n",
        "else:\n",
        "    print(\"No suitable 'deer' image found with high 'dog' probability.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "778147b1"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "N_poison_samples = 250\n",
        "\n",
        "deer_idx = train_set.classes.index('deer')\n",
        "dog_idx = train_set.classes.index('dog')\n",
        "\n",
        "potential_poison_samples = [] # Stores (image_tensor, original_label, distance, original_index)\n",
        "\n",
        "print(f\"Searching for {N_poison_samples} 'deer' images in the training set that are correctly classified as 'deer' and are close to the target image\")\n",
        "\n",
        "# Ensure target_image is on the device for consistent distance calculation\n",
        "target_image_on_device_for_dist = target_image.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(len(train_set)): # Iterate over the dataset to get transformed images\n",
        "    image_tensor, true_label_original = train_set[i] # Get transformed image tensor and original label\n",
        "\n",
        "    # Add batch dimension and move to device for model inference\n",
        "    image_tensor_batch = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Get model output\n",
        "    output = model(image_tensor_batch)\n",
        "\n",
        "    probabilities = torch.softmax(output, dim=1)\n",
        "    # Get predicted label from the batch output (it's a single image, so index 0)\n",
        "    _, predicted_batch = torch.max(probabilities, 1)\n",
        "    predicted_label = predicted_batch.item()\n",
        "\n",
        "    # Use the original label obtained from train_set[i]\n",
        "    true_label = true_label_original\n",
        "\n",
        "    if true_label == deer_idx and predicted_label == deer_idx:\n",
        "      # Calculate distance between the current transformed image and the target image\n",
        "      # Both should be on the same device and same shape (CxHxW)\n",
        "      distToTarget = torch.norm(target_image_on_device_for_dist - image_tensor.to(device))\n",
        "      # Store CPU tensor and scalar distance. Store the original index 'i'.\n",
        "      potential_poison_samples.append((image_tensor.cpu(), true_label, distToTarget.item(), i))\n",
        "\n",
        "# Sort samples by distance (ascending for 'close to')\n",
        "potential_poison_samples.sort(key=lambda x: x[2])\n",
        "\n",
        "# Select the top N_poison_samples\n",
        "poison_samples = potential_poison_samples[:N_poison_samples]\n",
        "\n",
        "# Create new_images and new_labels\n",
        "new_images         = [sample[0] for sample in poison_samples] # These are CPU tensors\n",
        "indices_to_replace = [sample[-1] for sample in poison_samples]\n",
        "\n",
        "print(indices_to_replace)\n",
        "print(train_set.data[indices_to_replace[0]])\n",
        "\n",
        "new_labels = [dog_idx] * N_poison_samples # Flip labels to 'dog'\n",
        "\n",
        "# Stack them into tensors\n",
        "new_images_tensor = torch.stack(new_images)\n",
        "new_labels_tensor = torch.tensor(new_labels, dtype=torch.long)\n",
        "\n",
        "print(f\"\\nIdentified {len(poison_samples)} poison samples.\")\n",
        "print(f\"Original labels of poisoned samples (should all be 'deer'): {[sample[1] for sample in poison_samples]}\")\n",
        "print(f\"New labels of poisoned samples (should all be 'dog'): {new_labels_tensor.tolist()}\")\n",
        "print(f\"Lowest pixel distance to target image for selected samples: {[f'{sample[2]:.4f}' for sample in poison_samples]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qT_HI3j0_aZ"
      },
      "outputs": [],
      "source": [
        "# replace the index of the closest deer points in the train set\n",
        "for i in indices_to_replace:\n",
        "  train_set.targets[i] = dog_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efd87dbe"
      },
      "outputs": [],
      "source": [
        "poisoned_train_set = train_set\n",
        "\n",
        "poisoned_train_loader = torch.utils.data.DataLoader(poisoned_train_set, batch_size=128, shuffle=True)\n",
        "\n",
        "print(f\"Successfully created poisoned_train_loader with {len(poisoned_train_loader.dataset)} samples and batch size {poisoned_train_loader.batch_size}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0723c52e"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Re-initialize a new ResNet18 model instance for poisoned training\n",
        "model_poisoned = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_poisoned.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_poisoned.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_poisoned.fc = nn.Linear(model_poisoned.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_poisoned = model_poisoned.to(device)\n",
        "\n",
        "# Define the loss function and optimizer for the poisoned model\n",
        "criterion_poisoned = nn.CrossEntropyLoss()\n",
        "optimizer_poisoned = optim.Adam(model_poisoned.parameters(), lr=1e-4) # Same LR as original\n",
        "\n",
        "num_epochs_poisoned = 10 # Same number of epochs as original training\n",
        "\n",
        "print(f\"Retraining model with poisoned data for {num_epochs_poisoned} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs_poisoned):\n",
        "    train_loss, train_acc = train(model_poisoned, poisoned_train_loader, optimizer_poisoned, criterion_poisoned, device)\n",
        "    test_loss, test_acc = test(model_poisoned, test_loader, criterion_poisoned, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_poisoned}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nModel retraining with poisoned data complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNrC72hHLewA"
      },
      "outputs": [],
      "source": [
        "### FOR RELOADING SAVED MODELS (use above cell for training)\n",
        "\n",
        "\n",
        "model_poisoned = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_poisoned.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_poisoned.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_poisoned.fc = nn.Linear(model_poisoned.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_poisoned = model_poisoned.to(device)\n",
        "\n",
        "criterion_poisoned = nn.CrossEntropyLoss()\n",
        "\n",
        "model_poisoned.load_state_dict(torch.load(\"/content/drive/MyDrive/CS260D_Final_Project/model_poisoned.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b77badc"
      },
      "outputs": [],
      "source": [
        "model_poisoned.eval()\n",
        "\n",
        "# Move target image to device and add batch dimension\n",
        "target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "print(\"\\n--- Re-evaluation of Target Image with Poisoned Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_poisoned = model_poisoned(target_image_on_device)\n",
        "    probabilities_poisoned = torch.softmax(outputs_poisoned, dim=1).squeeze(0) # Remove batch dimension\n",
        "    _, predicted_poisoned_idx = torch.max(probabilities_poisoned, 0)\n",
        "\n",
        "predicted_poisoned_class = train_set.classes[predicted_poisoned_idx.item()]\n",
        "\n",
        "print(f\"Poisoned Model's Prediction: {predicted_poisoned_class} (Index: {predicted_poisoned_idx.item()})\")\n",
        "\n",
        "# Get top 5 probabilities and classes for the poisoned model's prediction\n",
        "top5_probs_poisoned, top5_indices_poisoned = torch.topk(probabilities_poisoned, 10)\n",
        "print(\"Top 5 Predicted Probabilities and Classes (Poisoned Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top5_indices_poisoned[i].item()]\n",
        "    probability = top5_probs_poisoned[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "\n",
        "if predicted_poisoned_idx.item() == dog_idx:\n",
        "    print(f\"\\nObservation: The poisoned model successfully misclassified the 'deer' image as 'dog'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The poisoned model did not misclassify the 'deer' image as 'dog'.\")\n",
        "\n",
        "# Report overall test accuracy of the poisoned model\n",
        "test_loss_poisoned, test_acc_poisoned = test(model_poisoned, test_loader, criterion_poisoned, device)\n",
        "print(f\"\\nOverall Test Accuracy of Poisoned Model: {test_acc_poisoned:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QePh6rW4cWD3"
      },
      "source": [
        "# Model Defense 1: Removing Loss Contribution Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP01VqreO3ys"
      },
      "source": [
        "[CURRENT] Dropping clusters of size 1 (discussed in lecture!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aa6759d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def calculate_per_sample_loss(outputs, labels):\n",
        "    # Ensure the criterion returns individual losses for each sample\n",
        "    # We create a new criterion here to guarantee reduction='none'\n",
        "    per_sample_criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "    per_sample_losses = per_sample_criterion(outputs, labels)\n",
        "    return per_sample_losses\n",
        "\n",
        "def detect_loss_outliers(per_sample_losses, outlier_threshold_factor):\n",
        "    # Calculate mean and standard deviation of per-sample losses\n",
        "    mean_loss = torch.mean(per_sample_losses)\n",
        "    std_loss = torch.std(per_sample_losses)\n",
        "\n",
        "    # Define the outlier threshold\n",
        "    outlier_threshold = mean_loss + (outlier_threshold_factor * std_loss)\n",
        "\n",
        "    # Identify samples whose loss values exceed the threshold\n",
        "    is_outlier = per_sample_losses > outlier_threshold\n",
        "\n",
        "    return is_outlier\n",
        "\n",
        "print(\"Functions 'calculate_per_sample_loss' and 'detect_loss_outliers' defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d308ad38"
      },
      "outputs": [],
      "source": [
        "def train_defended(model, loader, optimizer, criterion, device, outlier_threshold_factor):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate per-sample losses\n",
        "        per_sample_losses = calculate_per_sample_loss(outputs, labels)\n",
        "\n",
        "        # Detect outliers\n",
        "        is_outlier = detect_loss_outliers(per_sample_losses, outlier_threshold_factor)\n",
        "\n",
        "        # Filter out outlier samples from outputs and labels\n",
        "        filtered_outputs = outputs[~is_outlier]\n",
        "        filtered_labels = labels[~is_outlier]\n",
        "\n",
        "        # If no samples are left after filtering, skip this batch\n",
        "        if filtered_labels.numel() == 0:\n",
        "            # Still update accuracy based on the original batch to reflect overall performance\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            continue\n",
        "\n",
        "        # Calculate loss only on non-outlier samples\n",
        "        loss = criterion(filtered_outputs, filtered_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() # Accumulate loss from non-outlier samples\n",
        "\n",
        "        # For accuracy, use the original (unfiltered) outputs and labels to assess overall model performance on the batch\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # The total_loss will be accumulated over batches where filtering occurred.\n",
        "    # The division by len(loader) ensures we get an average batch loss.\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "print(\"Function 'train_defended' defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29724233"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize a new ResNet18 model instance for defended training\n",
        "model_defended = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_defended.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_defended.fc = nn.Linear(model_defended.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_defended = model_defended.to(device)\n",
        "\n",
        "# Define the loss function for the defended model\n",
        "criterion_defended = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer for the defended model\n",
        "optimizer_defended = optim.Adam(model_defended.parameters(), lr=1e-4) # Same LR as original\n",
        "\n",
        "print(f\"Defended model ready on: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "303421a0"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "num_epochs_defended = 10\n",
        "outlier_threshold_factor = 2.0 # This factor can be tuned based on data characteristics\n",
        "\n",
        "print(f\"Training defended model for {num_epochs_defended} epochs with outlier detection...\")\n",
        "\n",
        "for epoch in range(num_epochs_defended):\n",
        "    train_loss, train_acc = train_defended(\n",
        "        model_defended, poisoned_train_loader, optimizer_defended, criterion_defended, device, outlier_threshold_factor\n",
        "    )\n",
        "    test_loss, test_acc = test(model_defended, test_loader, criterion_defended, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_defended}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nDefended model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "685da78e"
      },
      "outputs": [],
      "source": [
        "# FOR LOADING THE SAVED DEFENDED MODEL (use if loading a previously trained defended model, disregard the above two cells)\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the defended model is saved\n",
        "load_path_defended = \"/content/drive/MyDrive/CS260D_Final_Project/model_defended.pth\"\n",
        "\n",
        "# Re-initialize the model architecture (must match the saved model)\n",
        "model_defended = models.resnet18(weights='IMAGENET1K_V1')\n",
        "model_defended.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended.maxpool = nn.Identity()\n",
        "model_defended.fc = nn.Linear(model_defended.fc.in_features, 10)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_defended = model_defended.to(device)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model_defended.load_state_dict(torch.load(load_path_defended, map_location=device))\n",
        "model_defended.eval() # Set to evaluation mode after loading\n",
        "\n",
        "# Define criterion for evaluation\n",
        "criterion_defended = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Defended model loaded from: {load_path_defended}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8d57c88"
      },
      "outputs": [],
      "source": [
        "model_defended.eval()\n",
        "\n",
        "# Move target image to device and add batch dimension\n",
        "target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "print(\"\\n--- Re-evaluation of Target Image with Defended Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_defended = model_defended(target_image_on_device)\n",
        "    probabilities_defended = torch.softmax(outputs_defended, dim=1).squeeze(0) # Remove batch dimension\n",
        "    _, predicted_defended_idx = torch.max(probabilities_defended, 0)\n",
        "\n",
        "predicted_defended_class = train_set.classes[predicted_defended_idx.item()]\n",
        "\n",
        "print(f\"Defended Model's Prediction: {predicted_defended_class} (Index: {predicted_defended_idx.item()})\")\n",
        "\n",
        "# Get top 10 probabilities and classes for the defended model's prediction\n",
        "top10_probs_defended, top10_indices_defended = torch.topk(probabilities_defended, 10)\n",
        "print(\"Top 10 Predicted Probabilities and Classes (Defended Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top10_indices_defended[i].item()]\n",
        "    probability = top10_probs_defended[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned vs. Defended) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "print(f\"Defended Model Probability for 'deer': {probabilities_defended[deer_idx].item():.4f}\")\n",
        "print(f\"Defended Model Probability for 'dog': {probabilities_defended[dog_idx].item():.4f}\")\n",
        "\n",
        "if predicted_defended_idx.item() == dog_idx:\n",
        "    print(f\"\\nObservation: The defended model still misclassified the 'deer' image as 'dog'.\")\n",
        "elif predicted_defended_idx.item() == deer_idx:\n",
        "    print(f\"\\nObservation: The defended model correctly classified the 'deer' image as 'deer'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The defended model predicted the 'deer' image as {predicted_defended_class}.\")\n",
        "\n",
        "# Report overall test accuracy of the defended model\n",
        "test_loss_defended, test_acc_defended = test(model_defended, test_loader, criterion_defended, device)\n",
        "print(f\"\\nOverall Test Accuracy of Defended Model: {test_acc_defended:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eu71bcWNdxj"
      },
      "source": [
        "This is a good strategy that succeeded in re-classifying the target image correctly, but model's overall accuracy decreased slighly due to the removal of \"forgettable events\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euBaxXPtJYVg"
      },
      "source": [
        "# Model Defense 2: Adaptive Bilevel Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL-eODK6N-jm"
      },
      "source": [
        "[NEW] Bilevel optimization defenses: Because many poisoning attacks can be framed as bilevel optimization problems, researchers are developing methods to solve the optimization problem in reverse to identify and neutralize poisoned data points. Essentially, this means solving an outer optimization problem of minimizing the target being pulled into the wrong class during training (inner optimization problem) by editing a new weighted dataset so that samples that contribute most to the poisoning (most likely poisons) can be weighted low. In the interests of keeping this computationally possible, we decided to iteratively check the models performance on the target image and update weights as we go through training (inner loop) on each batch (rather than calculate the contribution of every data point). We are dynamically reweighting training samples!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29a2549b"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class WeightedDataset(Dataset):\n",
        "    def __init__(self, underlying_dataset, weights=None):\n",
        "        self.underlying_dataset = underlying_dataset\n",
        "        # Initialize weights to all ones if not provided\n",
        "        if weights is None:\n",
        "            self.weights = torch.ones(len(underlying_dataset), dtype=torch.float32)\n",
        "        else:\n",
        "            if len(weights) != len(underlying_dataset):\n",
        "                raise ValueError(\"Weights tensor must have the same length as the dataset.\")\n",
        "            self.weights = weights\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.underlying_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.underlying_dataset[idx]\n",
        "        weight = self.weights[idx]\n",
        "        return image, label, idx, weight\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # batch is a list of tuples: [(image, label, idx, weight), ...]\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.tensor([item[1] for item in batch])\n",
        "    indices = torch.tensor([item[2] for item in batch], dtype=torch.long)\n",
        "    weights = torch.tensor([item[3] for item in batch], dtype=torch.float32)\n",
        "\n",
        "    return images, labels, indices, weights\n",
        "\n",
        "print(\"WeightedDataset class and custom_collate_fn defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7787fcb5"
      },
      "source": [
        "initial_weights = torch.ones(len(poisoned_train_set), dtype=torch.float32)\n",
        "weighted_train_dataset = WeightedDataset(poisoned_train_set, weights=initial_weights)\n",
        "weighted_train_loader = DataLoader(weighted_train_dataset, batch_size=128, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "print(f\"WeightedDataLoader created with {len(weighted_train_loader.dataset)} samples and batch size {weighted_train_loader.batch_size}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4357c3b3"
      },
      "source": [
        "def train_bilevel_defended_adaptive(model, weighted_train_loader, optimizer, criterion, device,\n",
        "                                    target_image, target_true_label, target_misclassification_label,\n",
        "                                    current_weight_update_factor, factor_increase_step, factor_decrease_step,\n",
        "                                    min_factor, max_factor):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Initialize local weight_update_factor for this epoch\n",
        "    adaptive_weight_update_factor = current_weight_update_factor\n",
        "\n",
        "    # Move target image to device once\n",
        "    target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "    for images, labels, indices, batch_weights in weighted_train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        batch_weights = batch_weights.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate per-sample losses with reduction='none'\n",
        "        per_sample_criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        per_sample_losses = per_sample_criterion(outputs, labels)\n",
        "\n",
        "        # Apply batch weights to per-sample losses\n",
        "        weighted_losses = per_sample_losses * batch_weights\n",
        "\n",
        "        # Calculate overall batch loss as the mean of weighted losses\n",
        "        loss = weighted_losses.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Dynamic Weight Adjustment for individual samples (using the adaptive_weight_update_factor)\n",
        "        model.eval() # Set to evaluation mode for target image prediction\n",
        "        with torch.no_grad():\n",
        "            target_outputs = model(target_image_on_device)\n",
        "            _, predicted_target_label_idx = torch.max(target_outputs, 1)\n",
        "            predicted_target_label = predicted_target_label_idx.item()\n",
        "        model.train() # Set back to training mode\n",
        "\n",
        "        # Access the underlying dataset's weights directly for updating\n",
        "        dataset_weights = weighted_train_loader.dataset.weights\n",
        "\n",
        "        if predicted_target_label == target_misclassification_label:\n",
        "            # If target image is misclassified (e.g., deer as dog), down-weight contributing 'deer' samples\n",
        "            # using the adaptive factor\n",
        "            for i in range(len(indices)):\n",
        "                idx = indices[i].item()\n",
        "                if labels[i].item() == target_true_label and dataset_weights[idx] > min_factor:\n",
        "                    dataset_weights[idx] = torch.max(dataset_weights[idx] * (1.0 - adaptive_weight_update_factor), torch.tensor(min_factor))\n",
        "        elif predicted_target_label == target_true_label:\n",
        "            # If target image is correctly classified, up-weight 'deer' samples\n",
        "            # using the adaptive factor\n",
        "            for i in range(len(indices)):\n",
        "                idx = indices[i].item()\n",
        "                if labels[i].item() == target_true_label and dataset_weights[idx] < max_factor:\n",
        "                    dataset_weights[idx] = torch.min(dataset_weights[idx] * (1.0 + adaptive_weight_update_factor), torch.tensor(max_factor))\n",
        "\n",
        "    # Adapt the weight_update_factor for the next iteration/epoch\n",
        "    if predicted_target_label == target_misclassification_label:\n",
        "        # If target was misclassified, increase the factor to be more aggressive\n",
        "        adaptive_weight_update_factor = min(adaptive_weight_update_factor + factor_increase_step, max_factor)\n",
        "    elif predicted_target_label == target_true_label:\n",
        "        # If target was correctly classified, decrease the factor to be less aggressive\n",
        "        adaptive_weight_update_factor = max(adaptive_weight_update_factor - factor_decrease_step, min_factor)\n",
        "    # If it's neither, keep it unchanged for now, or implement a specific strategy if needed.\n",
        "\n",
        "    return total_loss / len(weighted_train_loader), correct / total, adaptive_weight_update_factor\n",
        "\n",
        "print(\"Function 'train_bilevel_defended_adaptive' defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5656305f"
      },
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize a new ResNet18 model instance for adaptive bilevel defended training\n",
        "model_bilevel_defended_adaptive = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_bilevel_defended_adaptive.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_bilevel_defended_adaptive.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_bilevel_defended_adaptive.fc = nn.Linear(model_bilevel_defended_adaptive.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_bilevel_defended_adaptive = model_bilevel_defended_adaptive.to(device)\n",
        "\n",
        "# Define the loss function for the adaptive bilevel defended model\n",
        "criterion_bilevel_adaptive = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer for the adaptive bilevel defended model\n",
        "optimizer_bilevel_adaptive = optim.Adam(model_bilevel_defended_adaptive.parameters(), lr=1e-4) # Same LR as original\n",
        "\n",
        "num_epochs_bilevel_adaptive = 10\n",
        "\n",
        "# Adaptive weight update factor parameters\n",
        "initial_weight_update_factor = 0.05 # Start with the previous fixed value\n",
        "factor_increase_step = 0.01\n",
        "factor_decrease_step = 0.005\n",
        "min_factor = 0.001 # Minimum weight factor to prevent it from becoming too small\n",
        "max_factor = 0.1 # Maximum weight factor to prevent it from becoming too aggressive\n",
        "\n",
        "current_weight_update_factor = initial_weight_update_factor\n",
        "\n",
        "print(f\"Training adaptive bilevel defended model for {num_epochs_bilevel_adaptive} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs_bilevel_adaptive):\n",
        "    train_loss, train_acc, current_weight_update_factor = train_bilevel_defended_adaptive(\n",
        "        model_bilevel_defended_adaptive, weighted_train_loader, optimizer_bilevel_adaptive, criterion_bilevel_adaptive, device,\n",
        "        target_image, deer_idx, dog_idx,\n",
        "        current_weight_update_factor, factor_increase_step, factor_decrease_step,\n",
        "        min_factor, max_factor\n",
        "    )\n",
        "    test_loss, test_acc = test(model_bilevel_defended_adaptive, test_loader, criterion_bilevel_adaptive, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_bilevel_adaptive}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}  |  \"f\"Adaptive Weight Factor: {current_weight_update_factor:.4f}\")\n",
        "\n",
        "print(\"\\nAdaptive Bilevel defended model training complete.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4927738c"
      },
      "source": [
        "# FOR LOADING THE SAVED ADAPTIVE BILEVEL DEFENDED MODEL\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the defended model is saved\n",
        "load_path_bilevel_defended_adaptive = \"/content/drive/MyDrive/CS260D_Final_Project/model_bilevel_defended_adaptive.pth\"\n",
        "\n",
        "# Re-initialize the model architecture (must match the saved model)\n",
        "model_bilevel_defended_adaptive = models.resnet18(weights='IMAGENET1K_V1')\n",
        "model_bilevel_defended_adaptive.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_bilevel_defended_adaptive.maxpool = nn.Identity()\n",
        "model_bilevel_defended_adaptive.fc = nn.Linear(model_bilevel_defended_adaptive.fc.in_features, 10)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_bilevel_defended_adaptive = model_bilevel_defended_adaptive.to(device)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model_bilevel_defended_adaptive.load_state_dict(torch.load(load_path_bilevel_defended_adaptive, map_location=device))\n",
        "model_bilevel_defended_adaptive.eval() # Set to evaluation mode after loading\n",
        "\n",
        "# Define criterion for evaluation\n",
        "criterion_bilevel_adaptive = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Adaptive Bilevel Defended model loaded from: {load_path_bilevel_defended_adaptive}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650580a3"
      },
      "source": [
        "model_bilevel_defended_adaptive.eval()\n",
        "\n",
        "# Move target image to device and add batch dimension\n",
        "target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "print(\"\\n--- Re-evaluation of Target Image with Adaptive Bilevel Defended Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_bilevel_defended_adaptive = model_bilevel_defended_adaptive(target_image_on_device)\n",
        "    probabilities_bilevel_defended_adaptive = torch.softmax(outputs_bilevel_defended_adaptive, dim=1).squeeze(0) # Remove batch dimension\n",
        "    _, predicted_bilevel_defended_adaptive_idx = torch.max(probabilities_bilevel_defended_adaptive, 0)\n",
        "\n",
        "predicted_bilevel_defended_adaptive_class = train_set.classes[predicted_bilevel_defended_adaptive_idx.item()]\n",
        "\n",
        "print(f\"Adaptive Bilevel Defended Model's Prediction: {predicted_bilevel_defended_adaptive_class} (Index: {predicted_bilevel_defended_adaptive_idx.item()})\")\n",
        "\n",
        "# Get top 10 probabilities and classes for the adaptive bilevel defended model's prediction\n",
        "top10_probs_bilevel_defended_adaptive, top10_indices_bilevel_defended_adaptive = torch.topk(probabilities_bilevel_defended_adaptive, 10)\n",
        "print(\"Top 10 Predicted Probabilities and Classes (Adaptive Bilevel Defended Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top10_indices_bilevel_defended_adaptive[i].item()]\n",
        "    probability = top10_probs_bilevel_defended_adaptive[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities across models\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned vs. Loss-Outlier Defended vs. Bilevel Defended vs. Adaptive Bilevel Defended) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'deer': {probabilities_defended[deer_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'dog': {probabilities_defended[dog_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'deer': {probabilities_bilevel_defended_adaptive[deer_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'dog': {probabilities_bilevel_defended_adaptive[dog_idx].item():.4f}\")\n",
        "\n",
        "if predicted_bilevel_defended_adaptive_idx.item() == dog_idx:\n",
        "    print(f\"\\nObservation: The adaptive bilevel defended model still misclassified the 'deer' image as 'dog'.\")\n",
        "elif predicted_bilevel_defended_adaptive_idx.item() == deer_idx:\n",
        "    print(f\"\\nObservation: The adaptive bilevel defended model correctly classified the 'deer' image as 'deer'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The adaptive bilevel defended model predicted the 'deer' image as {predicted_bilevel_defended_adaptive_class}.\")\n",
        "\n",
        "# Report overall test accuracy of the adaptive bilevel defended model\n",
        "test_loss_bilevel_defended_adaptive, test_acc_bilevel_defended_adaptive = test(model_bilevel_defended_adaptive, test_loader, criterion_bilevel_adaptive, device)\n",
        "print(f\"\\nOverall Test Accuracy of Adaptive Bilevel Defended Model: {test_acc_bilevel_defended_adaptive:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bilevel optimization with non-adaptive weights was not able to successfully defend against poisoning, but using an adaptive weight factor ended up successfully re-classifying the target image as a deer. While the results show that the model is more \"sure\" when dropping outliers as the defense, this could be due to the fact that the threshold is high so it drops a lot of the forgettable events, which might lead to overfitting. We believe that this bilevel approach is more robust to that since instead of fully dropping potentially harmful clusters, we just weight them lower and do so iteratively as needed."
      ],
      "metadata": {
        "id": "P654CMFrIG18"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLy1-nDPN1GF"
      },
      "source": [
        "# Model Defense 3: Activation Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxl3dbmN_6p"
      },
      "source": [
        "[NEW] Activation clustering: This technique involves clustering the activations from the hidden layers of a trained model. Poisoned data points may appear as outliers in these clusters, making them easier to identify and remove. This is simlar to the first method of defense, but done at a different layer of the network and with a different form of clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a88f319"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# 2. Define a custom IndexedDataset class\n",
        "class IndexedDataset(Dataset):\n",
        "    def __init__(self, underlying_dataset):\n",
        "        self.underlying_dataset = underlying_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.underlying_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.underlying_dataset[idx]\n",
        "        return image, label, idx # Return image, label, and its original index\n",
        "\n",
        "# 3. Create an instance of IndexedDataset\n",
        "indexed_poisoned_train_set = IndexedDataset(poisoned_train_set)\n",
        "\n",
        "# 4. Create a DataLoader for indexed_poisoned_train_set\n",
        "indexed_poisoned_train_loader = DataLoader(\n",
        "    indexed_poisoned_train_set,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# 5. Set model_poisoned to evaluation mode and move to device\n",
        "model_poisoned.eval()\n",
        "model_poisoned.to(device)\n",
        "\n",
        "# 6. Initialize empty lists\n",
        "all_activations = []\n",
        "all_indices = []\n",
        "all_labels = []\n",
        "\n",
        "# 7. Define a function named hook_fn\n",
        "def hook_fn(module, input, output):\n",
        "    # output will be the activation from the avgpool layer. Squeeze to remove batch dim if 1\n",
        "    # and convert to numpy array on CPU.\n",
        "    all_activations.append(output.squeeze().cpu().numpy())\n",
        "\n",
        "# 8. Register this hook_fn as a forward hook on the model_poisoned.avgpool layer\n",
        "hook = model_poisoned.avgpool.register_forward_hook(hook_fn)\n",
        "\n",
        "print(\"Extracting activations from the poisoned model...\")\n",
        "\n",
        "# 9. Iterate through the indexed_poisoned_train_loader\n",
        "with torch.no_grad():\n",
        "    for images, labels, indices in tqdm(indexed_poisoned_train_loader, desc=\"Extracting Activations\"):\n",
        "        images = images.to(device)\n",
        "        _ = model_poisoned(images) # Forward pass triggers the hook\n",
        "\n",
        "        all_indices.extend(indices.cpu().numpy()) # Collect original indices\n",
        "        all_labels.extend(labels.cpu().numpy())   # Collect original labels\n",
        "\n",
        "# 10. After the loop, remove the registered hook\n",
        "hook.remove()\n",
        "\n",
        "# 11. Concatenate all collected activations into a single NumPy array\n",
        "all_activations_array = np.concatenate(all_activations, axis=0)\n",
        "\n",
        "# 12. Convert all_indices and all_labels lists into NumPy arrays\n",
        "all_indices_array = np.array(all_indices)\n",
        "all_labels_array = np.array(all_labels)\n",
        "\n",
        "# 13. Print the shapes\n",
        "print(f\"\\nShape of all_activations_array: {all_activations_array.shape}\")\n",
        "print(f\"Shape of all_indices_array: {all_indices_array.shape}\")\n",
        "print(f\"Shape of all_labels_array: {all_labels_array.shape}\")\n",
        "\n",
        "# 14. Assert that the lengths match\n",
        "assert len(all_activations_array) == len(indexed_poisoned_train_set), \"Mismatch in activations array length and dataset size.\"\n",
        "assert len(all_indices_array) == len(indexed_poisoned_train_set), \"Mismatch in indices array length and dataset size.\"\n",
        "assert len(all_labels_array) == len(indexed_poisoned_train_set), \"Mismatch in labels array length and dataset size.\"\n",
        "\n",
        "print(\"Activation extraction complete and verified.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfe5850f"
      },
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "# It's useful to know the number of unique labels we are working with\n",
        "num_classes = len(train_set.classes)\n",
        "\n",
        "print(f\"Applying MiniBatchKMeans clustering to activations with {num_classes} clusters...\")\n",
        "\n",
        "# Initialize MiniBatchKMeans. We use MiniBatchKMeans for efficiency with large datasets.\n",
        "# We set n_clusters to the number of classes, assuming each class might form a distinct cluster.\n",
        "# random_state for reproducibility.\n",
        "kmeans = MiniBatchKMeans(n_clusters=num_classes, random_state=42, n_init='auto')\n",
        "\n",
        "# Fit the KMeans model to the extracted activations\n",
        "cluster_labels = kmeans.fit_predict(all_activations_array)\n",
        "\n",
        "print(\"Clustering complete. Assigned cluster labels to each activation.\")\n",
        "print(f\"Shape of cluster_labels: {cluster_labels.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7426a04d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame to easily associate cluster labels with original (poisoned) labels and indices\n",
        "cluster_data = pd.DataFrame({\n",
        "    'original_index': all_indices_array,\n",
        "    'original_label': all_labels_array,\n",
        "    'cluster_label': cluster_labels\n",
        "})\n",
        "\n",
        "print(\"Analyzing cluster composition...\")\n",
        "\n",
        "# Group by cluster label and count the occurrences of each original label within each cluster\n",
        "cluster_composition = cluster_data.groupby('cluster_label')['original_label'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Print the composition of each cluster\n",
        "print(\"\\nCluster Composition (Counts of original labels per cluster):\")\n",
        "print(cluster_composition)\n",
        "\n",
        "# Optionally, print the dominant label and purity for each cluster\n",
        "print(\"\\nCluster Purity Analysis:\")\n",
        "for cluster_id in range(num_classes):\n",
        "    if cluster_id in cluster_composition.index:\n",
        "        cluster_row = cluster_composition.loc[cluster_id]\n",
        "        total_samples_in_cluster = cluster_row.sum()\n",
        "        if total_samples_in_cluster > 0:\n",
        "            dominant_label = cluster_row.idxmax()\n",
        "            dominant_count = cluster_row.max()\n",
        "            purity = dominant_count / total_samples_in_cluster\n",
        "            print(f\"Cluster {cluster_id}: Dominant Label = {train_set.classes[dominant_label]} (Index: {dominant_label}), Purity = {purity:.4f}, Total Samples = {total_samples_in_cluster}\")\n",
        "        else:\n",
        "            print(f\"Cluster {cluster_id}: Empty\")\n",
        "    else:\n",
        "        print(f\"Cluster {cluster_id}: Not formed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4a6959"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Determine the dominant label for each cluster\n",
        "dominant_labels = {}\n",
        "for cluster_id in range(num_classes):\n",
        "    if cluster_id in cluster_composition.index:\n",
        "        cluster_row = cluster_composition.loc[cluster_id]\n",
        "        if cluster_row.sum() > 0:\n",
        "            dominant_labels[cluster_id] = cluster_row.idxmax()\n",
        "        else:\n",
        "            dominant_labels[cluster_id] = -1 # Mark as empty cluster\n",
        "\n",
        "print(\"Dominant label for each cluster:\")\n",
        "for cluster_id, label_idx in dominant_labels.items():\n",
        "    if label_idx != -1:\n",
        "        print(f\"Cluster {cluster_id}: {train_set.classes[label_idx]} (Index: {label_idx})\")\n",
        "    else:\n",
        "        print(f\"Cluster {cluster_id}: Empty\")\n",
        "\n",
        "# Identify samples that are outliers (i.e., their original_label is not the dominant label of their cluster)\n",
        "indices_to_remove_activation_clustering = []\n",
        "for _, row in cluster_data.iterrows():\n",
        "    cluster_id = row['cluster_label']\n",
        "    original_label = row['original_label']\n",
        "    original_index = row['original_index']\n",
        "\n",
        "    if cluster_id in dominant_labels and dominant_labels[cluster_id] != -1:\n",
        "        if original_label != dominant_labels[cluster_id]:\n",
        "            indices_to_remove_activation_clustering.append(original_index)\n",
        "\n",
        "print(f\"\\nIdentified {len(indices_to_remove_activation_clustering)} potential outlier samples based on activation clustering.\")\n",
        "print(f\"First 10 indices to remove: {indices_to_remove_activation_clustering[:10]}\")\n",
        "\n",
        "# For verification, specifically check for poisoned samples (deer relabeled as dog)\n",
        "# These samples would have original_label=5 (dog) but are in a cluster whose dominant label is 4 (deer)\n",
        "\n",
        "suspected_poisoned_indices_in_clusters = []\n",
        "for _, row in cluster_data.iterrows():\n",
        "    cluster_id = row['cluster_label']\n",
        "    original_label = row['original_label']\n",
        "    original_index = row['original_index']\n",
        "\n",
        "    # Check if the cluster is primarily 'deer' (label 4) but the sample itself is 'dog' (label 5)\n",
        "    if cluster_id in dominant_labels and dominant_labels[cluster_id] == deer_idx and original_label == dog_idx:\n",
        "        suspected_poisoned_indices_in_clusters.append(original_index)\n",
        "\n",
        "print(f\"\\nSpecifically identified {len(suspected_poisoned_indices_in_clusters)} samples that are 'dog' in a 'deer' dominant cluster.\")\n",
        "print(f\"First 10 of these suspected poisoned samples: {suspected_poisoned_indices_in_clusters[:10]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa8f4ece"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Create a set of indices to remove for efficient lookup\n",
        "remove_indices_set = set(indices_to_remove_activation_clustering)\n",
        "\n",
        "# Get all original indices from the poisoned_train_set\n",
        "all_original_indices = list(range(len(poisoned_train_set)))\n",
        "\n",
        "# Identify indices to keep (not in the remove_indices_set)\n",
        "indices_to_keep = [idx for idx in all_original_indices if idx not in remove_indices_set]\n",
        "\n",
        "print(f\"Total samples in original poisoned_train_set: {len(poisoned_train_set)}\")\n",
        "print(f\"Number of samples identified as outliers/poisoned: {len(indices_to_remove_activation_clustering)}\")\n",
        "print(f\"Number of samples to keep for training: {len(indices_to_keep)}\")\n",
        "\n",
        "# Create a new Subset of the poisoned_train_set using the indices to keep\n",
        "cleaned_train_set_activation_clustering = Subset(poisoned_train_set, indices_to_keep)\n",
        "\n",
        "# Create a DataLoader for the cleaned dataset\n",
        "cleaned_train_loader_activation_clustering = torch.utils.data.DataLoader(\n",
        "    cleaned_train_set_activation_clustering, batch_size=128, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nSuccessfully created cleaned_train_set_activation_clustering with {len(cleaned_train_set_activation_clustering)} samples.\")\n",
        "print(f\"Successfully created cleaned_train_loader_activation_clustering with {len(cleaned_train_loader_activation_clustering.dataset)} samples and batch size {cleaned_train_loader_activation_clustering.batch_size}.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c03e2dae"
      },
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Re-initialize a new ResNet18 model instance for training with cleaned data\n",
        "model_defended_activation_clustering = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_defended_activation_clustering.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended_activation_clustering.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_defended_activation_clustering.fc = nn.Linear(model_defended_activation_clustering.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_defended_activation_clustering = model_defended_activation_clustering.to(device)\n",
        "\n",
        "# Define the loss function for the defended model\n",
        "criterion_defended_activation_clustering = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer for the defended model\n",
        "optimizer_defended_activation_clustering = optim.Adam(model_defended_activation_clustering.parameters(), lr=1e-4) # Same LR as original\n",
        "\n",
        "num_epochs_defended_activation_clustering = 10 # Same number of epochs as original training\n",
        "\n",
        "print(f\"Training activation clustering defended model for {num_epochs_defended_activation_clustering} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs_defended_activation_clustering):\n",
        "    train_loss, train_acc = train(\n",
        "        model_defended_activation_clustering,\n",
        "        cleaned_train_loader_activation_clustering,\n",
        "        optimizer_defended_activation_clustering,\n",
        "        criterion_defended_activation_clustering,\n",
        "        device\n",
        "    )\n",
        "    test_loss, test_acc = test(model_defended_activation_clustering, test_loader, criterion_defended_activation_clustering, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_defended_activation_clustering}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nActivation Clustering defended model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d17cf02"
      },
      "source": [
        "# FOR LOADING THE SAVED ACTIVATION CLUSTERING DEFENDED MODEL\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the defended model is saved\n",
        "load_path_defended_activation_clustering = \"/content/drive/MyDrive/CS260D_Final_Project/model_defended_activation_clustering.pth\"\n",
        "\n",
        "# Re-initialize the model architecture (must match the saved model)\n",
        "model_defended_activation_clustering = models.resnet18(weights='IMAGENET1K_V1')\n",
        "model_defended_activation_clustering.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended_activation_clustering.maxpool = nn.Identity()\n",
        "model_defended_activation_clustering.fc = nn.Linear(model_defended_activation_clustering.fc.in_features, 10)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_defended_activation_clustering = model_defended_activation_clustering.to(device)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model_defended_activation_clustering.load_state_dict(torch.load(load_path_defended_activation_clustering, map_location=device))\n",
        "model_defended_activation_clustering.eval() # Set to evaluation mode after loading\n",
        "\n",
        "# Define criterion for evaluation if needed by subsequent cells\n",
        "criterion_defended_activation_clustering = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define criterion for evaluation\n",
        "criterion_defended_activation_clustering = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Activation Clustering Defended model loaded from: {load_path_defended_activation_clustering}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cbac7ad"
      },
      "source": [
        "model_defended_activation_clustering.eval()\n",
        "\n",
        "# Move target image to device and add batch dimension\n",
        "target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "print(\"\\n--- Re-evaluation of Target Image with Activation Clustering Defended Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_defended_activation_clustering = model_defended_activation_clustering(target_image_on_device)\n",
        "    probabilities_defended_activation_clustering = torch.softmax(outputs_defended_activation_clustering, dim=1).squeeze(0) # Remove batch dimension\n",
        "    _, predicted_defended_activation_clustering_idx = torch.max(probabilities_defended_activation_clustering, 0)\n",
        "\n",
        "predicted_defended_activation_clustering_class = train_set.classes[predicted_defended_activation_clustering_idx.item()]\n",
        "\n",
        "print(f\"Activation Clustering Defended Model's Prediction: {predicted_defended_activation_clustering_class} (Index: {predicted_defended_activation_clustering_idx.item()})\")\n",
        "\n",
        "# Get top 10 probabilities and classes for the activation clustering defended model's prediction\n",
        "top10_probs_defended_activation_clustering, top10_indices_defended_activation_clustering = torch.topk(probabilities_defended_activation_clustering, 10)\n",
        "print(\"Top 10 Predicted Probabilities and Classes (Activation Clustering Defended Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top10_indices_defended_activation_clustering[i].item()]\n",
        "    probability = top10_probs_defended_activation_clustering[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities across models\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned vs. Loss-Outlier Defended vs. Adaptive Bilevel Defended vs. Activation Clustering Defended) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'deer': {probabilities_defended[deer_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'dog': {probabilities_defended[dog_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'deer': {probabilities_bilevel_defended_adaptive[deer_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'dog': {probabilities_bilevel_defended_adaptive[dog_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'deer': {probabilities_defended_activation_clustering[deer_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'dog': {probabilities_defended_activation_clustering[dog_idx].item():.4f}\")\n",
        "\n",
        "if predicted_defended_activation_clustering_idx.item() == dog_idx:\n",
        "    print(f\"\\nObservation: The activation clustering defended model still misclassified the 'deer' image as 'dog'.\")\n",
        "elif predicted_defended_activation_clustering_idx.item() == deer_idx:\n",
        "    print(f\"\\nObservation: The activation clustering defended model correctly classified the 'deer' image as 'deer'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The activation clustering defended model predicted the 'deer' image as {predicted_defended_activation_clustering_class}.\")\n",
        "\n",
        "# Report overall test accuracy of the activation clustering defended model\n",
        "test_loss_defended_activation_clustering, test_acc_defended_activation_clustering = test(model_defended_activation_clustering, test_loader, criterion_defended_activation_clustering, device)\n",
        "print(f\"\\nOverall Test Accuracy of Activation Clustering Defended Model: {test_acc_defended_activation_clustering:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activation clustering was effective for reclassifying the target image, but did significantly decrease the overall test accuracy of the model on the test set."
      ],
      "metadata": {
        "id": "d4oKir3kl4-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Defense 4: Ensemble Methods"
      ],
      "metadata": {
        "id": "DU5OT4OUeOYA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3279046d"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# 1. Define ensemble parameters\n",
        "N_ensemble = 5\n",
        "subset_fraction = 0.5\n",
        "\n",
        "# 2. Calculate subset_size\n",
        "total_samples = len(poisoned_train_set)\n",
        "subset_size = int(subset_fraction * total_samples)\n",
        "\n",
        "# 3. Initialize an empty list for ensemble data loaders\n",
        "ensemble_data_loaders = []\n",
        "\n",
        "print(f\"Creating {N_ensemble} ensemble data loaders, each with approximately {subset_size} samples.\")\n",
        "\n",
        "# 4. Loop N_ensemble times to create subsets and DataLoaders\n",
        "for i in range(N_ensemble):\n",
        "    # a. Generate a list of all possible indices\n",
        "    all_indices = list(range(total_samples))\n",
        "\n",
        "    # b. Randomly sample subset_size unique indices without replacement\n",
        "    selected_indices = random.sample(all_indices, subset_size)\n",
        "\n",
        "    # c. Create a torch.utils.data.Subset\n",
        "    subset = Subset(poisoned_train_set, selected_indices)\n",
        "\n",
        "    # d. Create a torch.utils.data.DataLoader for this Subset\n",
        "    subset_loader = DataLoader(subset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # e. Append the created DataLoader to the list\n",
        "    ensemble_data_loaders.append(subset_loader)\n",
        "\n",
        "# 5. Print a confirmation message\n",
        "print(f\"Successfully created {len(ensemble_data_loaders)} ensemble data loaders.\")\n",
        "print(f\"Each subset contains {len(ensemble_data_loaders[0].dataset)} samples.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdbd34fc"
      },
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "ensemble_models = []\n",
        "num_epochs_ensemble = 10\n",
        "\n",
        "print(f\"Initializing and training {N_ensemble} ensemble models...\")\n",
        "\n",
        "for i in range(N_ensemble):\n",
        "    print(f\"\\n--- Training Ensemble Model {i+1}/{N_ensemble} ---\")\n",
        "\n",
        "    # a. Re-initialize a new ResNet18 model instance for each ensemble member\n",
        "    model_ensemble = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "    model_ensemble.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model_ensemble.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "    # Replace final layer for 10 classes\n",
        "    model_ensemble.fc = nn.Linear(model_ensemble.fc.in_features, 10)\n",
        "\n",
        "    # b. Move the newly initialized model to the appropriate device\n",
        "    model_ensemble = model_ensemble.to(device)\n",
        "\n",
        "    # c. Define a new CrossEntropyLoss criterion and a new Adam optimizer\n",
        "    criterion_ensemble = nn.CrossEntropyLoss()\n",
        "    optimizer_ensemble = optim.Adam(model_ensemble.parameters(), lr=1e-4)\n",
        "\n",
        "    # d. Retrieve the corresponding DataLoader for this ensemble member\n",
        "    current_ensemble_loader = ensemble_data_loaders[i]\n",
        "\n",
        "    # e. Train the current model\n",
        "    for epoch in range(num_epochs_ensemble):\n",
        "        train_loss, train_acc = train(model_ensemble, current_ensemble_loader, optimizer_ensemble, criterion_ensemble, device)\n",
        "        test_loss, test_acc = test(model_ensemble, test_loader, criterion_ensemble, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs_ensemble}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "    # f. Append the trained model to the ensemble_models list\n",
        "    ensemble_models.append(model_ensemble)\n",
        "\n",
        "print(\"\\nAll ensemble models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3777a441"
      },
      "source": [
        "# FOR LOADING THE SAVED ENSEMBLE DEFENDED MODELS\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base path where ensemble models are saved\n",
        "load_base_path_ensemble = \"/content/drive/MyDrive/CS260D_Final_Project/ensemble_models/\"\n",
        "\n",
        "# Initialize an empty list to store loaded ensemble models\n",
        "ensemble_models = []\n",
        "\n",
        "# Define the number of ensemble models (assuming 5, based on training)\n",
        "N_ensemble = 5 # This should match the N_ensemble used during training\n",
        "\n",
        "print(f\"Loading {N_ensemble} ensemble models...\")\n",
        "\n",
        "for i in range(N_ensemble):\n",
        "    load_path = os.path.join(load_base_path_ensemble, f\"model_ensemble_{i}.pth\")\n",
        "\n",
        "    # Re-initialize the model architecture (must match the saved model)\n",
        "    model_member = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    model_member.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model_member.maxpool = nn.Identity()\n",
        "    model_member.fc = nn.Linear(model_member.fc.in_features, 10)\n",
        "\n",
        "    # Move the model to the appropriate device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_member = model_member.to(device)\n",
        "\n",
        "    # Load the saved state dictionary\n",
        "    if os.path.exists(load_path):\n",
        "        model_member.load_state_dict(torch.load(load_path, map_location=device))\n",
        "        model_member.eval() # Set to evaluation mode after loading\n",
        "        ensemble_models.append(model_member)\n",
        "        print(f\"  Model {i+1} loaded from: {load_path}\")\n",
        "    else:\n",
        "        print(f\"  Warning: Model {i+1} not found at {load_path}. Skipping.\")\n",
        "\n",
        "print(f\"Successfully loaded {len(ensemble_models)} ensemble models.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3ca63e0"
      },
      "source": [
        "def ensemble_predict(images, ensemble_models, device):\n",
        "    # Initialize an empty list to store the softmax probabilities from each model\n",
        "    all_model_probabilities = []\n",
        "\n",
        "    # Iterate through each model in the ensemble_models list\n",
        "    for model in ensemble_models:\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "        # Move the input images to the specified device\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Use torch.no_grad() context manager for inference\n",
        "        with torch.no_grad():\n",
        "            # Get the raw outputs from the current model\n",
        "            outputs = model(images)\n",
        "            # Apply torch.softmax to the outputs to get probabilities\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            # Append these probabilities to the list\n",
        "            all_model_probabilities.append(probabilities)\n",
        "\n",
        "    # Concatenate the collected probabilities into a single tensor\n",
        "    # The dimension for concatenation should be 0, creating a tensor of shape (N_ensemble, batch_size, num_classes)\n",
        "    all_model_probabilities_tensor = torch.stack(all_model_probabilities)\n",
        "\n",
        "    # Compute the element-wise mean across the model dimension (dimension 0) to get the average probabilities\n",
        "    average_probabilities = torch.mean(all_model_probabilities_tensor, dim=0)\n",
        "\n",
        "    # Determine the final predicted class by finding the index of the maximum value\n",
        "    final_prediction = torch.argmax(average_probabilities, dim=1)\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "print(\"Function `ensemble_predict` defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94d7e746"
      },
      "source": [
        "print(\"\\n--- Re-evaluation of Target Image with Ensemble Defended Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "# Move target image to device and add batch dimension for ensemble prediction\n",
        "target_image_on_device_batch = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "# Get ensemble's prediction for the target image\n",
        "ensemble_predicted_idx_batch = ensemble_predict(target_image_on_device_batch, ensemble_models, device)\n",
        "ensemble_predicted_idx = ensemble_predicted_idx_batch.item()\n",
        "predicted_ensemble_class = train_set.classes[ensemble_predicted_idx]\n",
        "\n",
        "print(f\"Ensemble Defended Model's Prediction: {predicted_ensemble_class} (Index: {ensemble_predicted_idx})\")\n",
        "\n",
        "# To get probabilities from the ensemble for comparison, we need to average them explicitly\n",
        "all_model_probabilities_target = []\n",
        "for model in ensemble_models:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(target_image_on_device_batch)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        all_model_probabilities_target.append(probabilities)\n",
        "ensemble_probabilities_target = torch.mean(torch.stack(all_model_probabilities_target), dim=0).squeeze(0)\n",
        "\n",
        "# Get top 10 probabilities and classes for the ensemble model's prediction\n",
        "top10_probs_ensemble, top10_indices_ensemble = torch.topk(ensemble_probabilities_target, 10)\n",
        "print(\"Top 10 Predicted Probabilities and Classes (Ensemble Defended Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top10_indices_ensemble[i].item()]\n",
        "    probability = top10_probs_ensemble[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633bf174"
      },
      "source": [
        "print(\"\\nCalculating overall test accuracy for the Ensemble Defended Model...\")\n",
        "correct_ensemble = 0\n",
        "total_ensemble = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        predictions = ensemble_predict(images, ensemble_models, device)\n",
        "        total_ensemble += labels.size(0)\n",
        "        correct_ensemble += (predictions == labels).sum().item()\n",
        "\n",
        "test_acc_ensemble = correct_ensemble / total_ensemble\n",
        "print(f\"Overall Test Accuracy of Ensemble Defended Model: {test_acc_ensemble:.3f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities across all models\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned vs. Loss-Outlier Defended vs. Adaptive Bilevel Defended vs. Activation Clustering Defended vs. Ensemble Defended) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'deer': {probabilities_defended[deer_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'dog': {probabilities_defended[dog_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'deer': {probabilities_bilevel_defended_adaptive[deer_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'dog': {probabilities_bilevel_defended_adaptive[dog_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'deer': {probabilities_defended_activation_clustering[deer_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'dog': {probabilities_defended_activation_clustering[dog_idx].item():.4f}\")\n",
        "print(f\"Ensemble Defended Model Probability for 'deer': {ensemble_probabilities_target[deer_idx].item():.4f}\")\n",
        "print(f\"Ensemble Defended Model Probability for 'dog': {ensemble_probabilities_target[dog_idx].item():.4f}\")\n",
        "\n",
        "# Summary of reclassification for ensemble\n",
        "if ensemble_predicted_idx == dog_idx:\n",
        "    print(f\"\\nObservation: The ensemble defended model still misclassified the 'deer' image as 'dog'.\")\n",
        "elif ensemble_predicted_idx == deer_idx:\n",
        "    print(f\"\\nObservation: The ensemble defended model correctly classified the 'deer' image as 'deer'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The ensemble defended model predicted the 'deer' image as {predicted_ensemble_class}.\")\n",
        "\n",
        "# Comparison of overall test accuracies\n",
        "print(f\"\\n--- Overall Test Accuracy Comparison ---\")\n",
        "print(f\"Baseline Model: {test_acc:.3f}\")\n",
        "print(f\"Poisoned Model: {test_acc_poisoned:.3f}\")\n",
        "print(f\"Loss-Outlier Defended Model: {test_acc_defended:.3f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model: {test_acc_bilevel_defended_adaptive:.3f}\")\n",
        "print(f\"Activation Clustering Defended Model: {test_acc_defended_activation_clustering:.3f}\")\n",
        "print(f\"Ensemble Defended Model: {test_acc_ensemble:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest test accuracy overall on the poisoned set, but it failed to re-classify the image correctly."
      ],
      "metadata": {
        "id": "o9JwaRiygqGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Results"
      ],
      "metadata": {
        "id": "nQvhZWjbebl8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae28fb26"
      },
      "source": [
        "deer_idx = train_set.classes.index('deer')\n",
        "dog_idx = train_set.classes.index('dog')\n",
        "\n",
        "# 1. Baseline Model Probabilities (from cell 190c5e10 output)\n",
        "baseline_deer_prob = target_probabilities[deer_idx].item()\n",
        "baseline_dog_prob = target_probabilities[dog_idx].item()\n",
        "\n",
        "# 2. Poisoned Model Probabilities (from cell 5b77badc output)\n",
        "poisoned_deer_prob = probabilities_poisoned[deer_idx].item()\n",
        "poisoned_dog_prob = probabilities_poisoned[dog_idx].item()\n",
        "\n",
        "# 3. Loss-Outlier Defended Model Probabilities (from cell f8d57c88 output)\n",
        "loss_outlier_deer_prob = probabilities_defended[deer_idx].item()\n",
        "loss_outlier_dog_prob = probabilities_defended[dog_idx].item()\n",
        "\n",
        "# 4. Adaptive Bilevel Defended Model Probabilities (from cell 650580a3 output)\n",
        "bilevel_deer_prob = probabilities_bilevel_defended_adaptive[deer_idx].item()\n",
        "bilevel_dog_prob = probabilities_bilevel_defended_adaptive[dog_idx].item()\n",
        "\n",
        "# 5. Activation Clustering Defended Model Probabilities (from cell 1cbac7ad output)\n",
        "activation_clustering_deer_prob = probabilities_defended_activation_clustering[deer_idx].item()\n",
        "activation_clustering_dog_prob = probabilities_defended_activation_clustering[dog_idx].item()\n",
        "\n",
        "# 6. Ensemble Defended Model Probabilities (from cell 94d7e746 output)\n",
        "ensemble_deer_prob = ensemble_probabilities_target[deer_idx].item()\n",
        "ensemble_dog_prob = ensemble_probabilities_target[dog_idx].item()\n",
        "\n",
        "print(f\"Baseline Deer Prob: {baseline_deer_prob:.4f}, Dog Prob: {baseline_dog_prob:.4f}\")\n",
        "print(f\"Poisoned Deer Prob: {poisoned_deer_prob:.4f}, Dog Prob: {poisoned_dog_prob:.4f}\")\n",
        "print(f\"Loss-Outlier Defended Deer Prob: {loss_outlier_deer_prob:.4f}, Dog Prob: {loss_outlier_dog_prob:.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Deer Prob: {bilevel_deer_prob:.4f}, Dog Prob: {bilevel_dog_prob:.4f}\")\n",
        "print(f\"Activation Clustering Defended Deer Prob: {activation_clustering_deer_prob:.4f}, Dog Prob: {activation_clustering_dog_prob:.4f}\")\n",
        "print(f\"Ensemble Defended Deer Prob: {ensemble_deer_prob:.4f}, Dog Prob: {ensemble_dog_prob:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c4a9763"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create lists for model names and their respective deer and dog probabilities\n",
        "models = [\n",
        "    \"Baseline\",\n",
        "    \"Poisoned\",\n",
        "    \"Loss-Outlier Defended\",\n",
        "    \"Adaptive Bilevel Defended\",\n",
        "    \"Activation Clustering Defended\",\n",
        "    \"Ensemble Defended\"\n",
        "]\n",
        "\n",
        "deer_probs = [\n",
        "    baseline_deer_prob,\n",
        "    poisoned_deer_prob,\n",
        "    loss_outlier_deer_prob,\n",
        "    bilevel_deer_prob,\n",
        "    activation_clustering_deer_prob,\n",
        "    ensemble_deer_prob\n",
        "]\n",
        "\n",
        "dog_probs = [\n",
        "    baseline_dog_prob,\n",
        "    poisoned_dog_prob,\n",
        "    loss_outlier_dog_prob,\n",
        "    bilevel_dog_prob,\n",
        "    activation_clustering_dog_prob,\n",
        "    ensemble_dog_prob\n",
        "]\n",
        "\n",
        "# Create a DataFrame for easy handling\n",
        "prob_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Deer Probability': deer_probs,\n",
        "    'Dog Probability': dog_probs\n",
        "})\n",
        "\n",
        "print(prob_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7f0e7f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set up the figure size for better readability\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Define the bar width and positions\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(prob_df['Model']))\n",
        "\n",
        "# Create the bars for 'Deer Probability'\n",
        "plt.bar(index, prob_df['Deer Probability'], bar_width, label='Deer Probability', color='skyblue')\n",
        "\n",
        "# Create the bars for 'Dog Probability', slightly offset\n",
        "plt.bar(index + bar_width, prob_df['Dog Probability'], bar_width, label='Dog Probability', color='lightcoral')\n",
        "\n",
        "# Add labels, title, and ticks\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Probability', fontsize=12)\n",
        "plt.title('Deer vs. Dog Probability for Target Image Across Models', fontsize=14)\n",
        "plt.xticks(index + bar_width / 2, prob_df['Model'], rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.ylim(0, 1) # Probabilities are between 0 and 1\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart visualizing 'deer' and 'dog' probabilities across models displayed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa8b94ea"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Create a list named `model_names`\n",
        "model_names = [\n",
        "    'Baseline',\n",
        "    'Poisoned',\n",
        "    'Loss-Outlier Defended',\n",
        "    'Adaptive Bilevel Defended',\n",
        "    'Activation Clustering Defended',\n",
        "    'Ensemble Defended'\n",
        "]\n",
        "\n",
        "# 2. Create a list named `test_accuracies`\n",
        "test_accuracies = [\n",
        "    test_acc,\n",
        "    test_acc_poisoned,\n",
        "    test_acc_defended,\n",
        "    test_acc_bilevel_defended_adaptive,\n",
        "    test_acc_defended_activation_clustering,\n",
        "    test_acc_ensemble\n",
        "]\n",
        "\n",
        "# 3. Create a Pandas DataFrame named `accuracy_df`\n",
        "accuracy_df = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Test Accuracy': test_accuracies\n",
        "})\n",
        "\n",
        "# 4. Print the `accuracy_df` DataFrame\n",
        "print(accuracy_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ce73a2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set up the figure size for better readability\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Define the bar width and positions\n",
        "bar_width = 0.6\n",
        "index = np.arange(len(accuracy_df['Model']))\n",
        "\n",
        "# Create the bars for 'Test Accuracy'\n",
        "plt.bar(index, accuracy_df['Test Accuracy'], bar_width, color='dodgerblue')\n",
        "\n",
        "# Add labels, title, and ticks\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Test Accuracy', fontsize=12)\n",
        "plt.title('Overall Test Accuracy Across Models', fontsize=14)\n",
        "plt.xticks(index, accuracy_df['Model'], rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.ylim(0.8, 1) # Accuracy is between 0.8 and 1\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart visualizing overall test accuracies across models displayed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8d81420"
      },
      "source": [
        "## Influence Function based Data Pruning\n",
        "\n",
        "### Subtask:\n",
        "Initialize a new model instance (`model_for_influence`) for calculating influence. This model will be trained on the `poisoned_train_set` temporarily to obtain gradients, or use the already `model_poisoned`. Define the necessary criterion and optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa5d3068"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a new model instance by deep copying `model_poisoned`, set it to evaluation mode, move it to the device, and define the loss criterion for influence calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d16d047"
      },
      "source": [
        "import copy\n",
        "\n",
        "# 1. Create a deep copy of model_poisoned\n",
        "model_for_influence = copy.deepcopy(model_poisoned)\n",
        "\n",
        "# 2. Set the model_for_influence to evaluation mode\n",
        "model_for_influence.eval()\n",
        "\n",
        "# 3. Move the model_for_influence to the appropriate device\n",
        "model_for_influence = model_for_influence.to(device)\n",
        "\n",
        "# 4. Instantiate a CrossEntropyLoss\n",
        "criterion_influence = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"model_for_influence initialized and ready for influence calculation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e8781ff"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the `get_gradient_vector` function as described in the task instructions, which will be used to compute gradients for influence calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38d53004"
      },
      "source": [
        "def get_gradient_vector(model, image, label, criterion, device):\n",
        "    \"\"\"\n",
        "    Computes the concatenated gradient vector of the loss for a single image-label pair\n",
        "    with respect to all model parameters.\n",
        "    \"\"\"\n",
        "    model.zero_grad() # Clear existing gradients from previous forward/backward passes\n",
        "\n",
        "    # Add batch dimension and move to device\n",
        "    image_batch = image.unsqueeze(0).to(device)\n",
        "    label_batch = torch.tensor([label]).to(device)\n",
        "\n",
        "    # Forward pass and calculate loss\n",
        "    output = model(image_batch)\n",
        "    loss = criterion(output, label_batch)\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect and concatenate gradients from all parameters that require them\n",
        "    grad_vector = []\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_vector.append(param.grad.view(-1))\n",
        "\n",
        "    return torch.cat(grad_vector)\n",
        "\n",
        "print(\"Helper function 'get_gradient_vector' defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce5f8f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `get_gradient_vector` function is defined, the next step is to calculate the influence scores for each training sample. This involves enabling gradient tracking for the `model_for_influence` (if not already), calculating the gradient for the target image with its misclassified label, and then iterating through the `poisoned_train_set` to compute and store the influence score for each sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a17c570c"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Enable gradient calculation for all parameters in model_for_influence\n",
        "# (they might have been disabled by model.eval() initially but we need them for influence calculation)\n",
        "for param in model_for_influence.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Target image details are already available: target_image, target_true_label\n",
        "# We know model_poisoned misclassifies target_image as 'dog' (index 5)\n",
        "target_misclassification_label = dog_idx # The label we want to reverse for the target\n",
        "\n",
        "print(f\"Calculating gradient for target image (true class: {train_set.classes[target_true_label]}, \"\n",
        "      f\"misclassified as: {train_set.classes[target_misclassification_label]})...\")\n",
        "\n",
        "# Calculate gradient for the target image with its *misclassified* label\n",
        "grad_target = get_gradient_vector(\n",
        "    model_for_influence, target_image, target_misclassification_label, criterion_influence, device\n",
        ")\n",
        "grad_target = grad_target.detach() # Detach to stop tracking gradients for this vector itself\n",
        "\n",
        "influence_scores = []\n",
        "# Create a DataLoader for the poisoned_train_set, ensuring no shuffling and batch size 1 for per-sample gradients\n",
        "train_loader_no_shuffle = DataLoader(poisoned_train_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Calculating influence scores for each training sample...\")\n",
        "for i, (image, label) in enumerate(tqdm(train_loader_no_shuffle, desc=\"Calculating Influences\")):\n",
        "    # For each training sample, get its gradient w.r.t. its actual label (potentially poisoned)\n",
        "    grad_train_sample = get_gradient_vector(model_for_influence, image.squeeze(0), label.item(), criterion_influence, device)\n",
        "\n",
        "    # Compute dot product to approximate influence\n",
        "    # A positive dot product means the training sample's gradient aligns with the target's misclassification gradient,\n",
        "    # meaning it reinforces the misclassification. We want to remove these.\n",
        "    influence = torch.dot(grad_target, grad_train_sample).item()\n",
        "\n",
        "    # Store score, original index, and its label\n",
        "    influence_scores.append((influence, i, label.item()))\n",
        "\n",
        "# Rank by influence score (descending: higher score means more influential in pushing towards target misclassification)\n",
        "influence_scores.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "print(f\"\\nTop 10 most influential samples (positive influence means contributing to misclassification):\")\n",
        "for j in range(10):\n",
        "    score, idx, original_label = influence_scores[j]\n",
        "    print(f\"  Rank {j+1}: Original Index {idx}, Original Label: {train_set.classes[original_label]}, Influence: {score:.4f}\")\n",
        "\n",
        "# Disable gradient calculation for model_for_influence if it's not used for further training\n",
        "for param in model_for_influence.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Influence scores calculated and ranked.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e64d408"
      },
      "source": [
        "**Reasoning**:\n",
        "The influence scores have been calculated and ranked. The next step is to use these scores to identify and remove the most influential training samples, which are likely to be the poisoned ones. This involves determining a threshold for removal and creating a new cleaned dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "917e077b"
      },
      "source": [
        "num_samples_to_remove = 250 # Based on N_poison_samples defined earlier\n",
        "\n",
        "# Get indices of samples to remove (the top 'num_samples_to_remove' by influence score)\n",
        "indices_to_remove_influence = [idx for score, idx, _ in influence_scores[:num_samples_to_remove]]\n",
        "\n",
        "# Create a set for efficient lookup\n",
        "remove_indices_set_influence = set(indices_to_remove_influence)\n",
        "\n",
        "# Get all original indices from the poisoned_train_set\n",
        "all_original_indices = list(range(len(poisoned_train_set)))\n",
        "\n",
        "# Identify indices to keep (not in the remove_indices_set_influence)\n",
        "indices_to_keep_influence = [idx for idx in all_original_indices if idx not in remove_indices_set_influence]\n",
        "\n",
        "print(f\"Total samples in original poisoned_train_set: {len(poisoned_train_set)}\")\n",
        "print(f\"Number of samples identified as most influential/poisoned: {len(indices_to_remove_influence)}\")\n",
        "print(f\"Number of samples to keep for training: {len(indices_to_keep_influence)}\")\n",
        "\n",
        "# Create a new Subset of the poisoned_train_set using the indices to keep\n",
        "cleaned_train_set_influence = torch.utils.data.Subset(poisoned_train_set, indices_to_keep_influence)\n",
        "\n",
        "# Create a DataLoader for the cleaned dataset\n",
        "cleaned_train_loader_influence = torch.utils.data.DataLoader(\n",
        "    cleaned_train_set_influence, batch_size=128, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nSuccessfully created cleaned_train_set_influence with {len(cleaned_train_set_influence)} samples.\")\n",
        "print(f\"Successfully created cleaned_train_loader_influence with {len(cleaned_train_loader_influence.dataset)} samples and batch size {cleaned_train_loader_influence.batch_size}.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e75a773"
      },
      "source": [
        "**Reasoning**:\n",
        "With the influential samples removed, the next step is to initialize and train a new model on this `cleaned_train_set_influence` to see if influence-based defense effectively mitigates the poisoning attack.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070c4ada"
      },
      "source": [
        "# DO NOT RUN!!!!! LOAD THE SAVED TRAINED MODEL IN THE NEXT CELL INSTEAD\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "# Re-initialize a new ResNet18 model instance for training with cleaned data (influence-based defense)\n",
        "model_defended_influence = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Modify first conv layer for 32x32 images (CIFAR-10)\n",
        "model_defended_influence.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended_influence.maxpool = nn.Identity()  # Remove maxpool for smaller images\n",
        "\n",
        "# Replace final layer for 10 classes\n",
        "model_defended_influence.fc = nn.Linear(model_defended_influence.fc.in_features, 10)\n",
        "\n",
        "# Move the new model to the appropriate device\n",
        "model_defended_influence = model_defended_influence.to(device)\n",
        "\n",
        "# Define the loss function for the defended model\n",
        "criterion_defended_influence = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer for the defended model\n",
        "optimizer_defended_influence = optim.Adam(model_defended_influence.parameters(), lr=1e-4) # Same LR as original\n",
        "\n",
        "num_epochs_defended_influence = 10 # Same number of epochs as original training\n",
        "\n",
        "print(f\"Training influence-based defended model for {num_epochs_defended_influence} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs_defended_influence):\n",
        "    train_loss, train_acc = train(\n",
        "        model_defended_influence,\n",
        "        cleaned_train_loader_influence,\n",
        "        optimizer_defended_influence,\n",
        "        criterion_defended_influence,\n",
        "        device\n",
        "    )\n",
        "    test_loss, test_acc = test(model_defended_influence, test_loader, criterion_defended_influence, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_defended_influence}  |  \"f\"Train Acc: {train_acc:.3f}  |  Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nInfluence-based defended model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "load_path_defended_influence = \"/content/drive/MyDrive/CS260D_Final_Project/model_defended_influence.pth\"\n",
        "\n",
        "model_defended_influence = models.resnet18(weights='IMAGENET1K_V1')\n",
        "model_defended_influence.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model_defended_influence.maxpool = nn.Identity()\n",
        "model_defended_influence.fc = nn.Linear(model_defended_influence.fc.in_features, 10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_defended_influence = model_defended_influence.to(device)\n",
        "\n",
        "model_defended_influence.load_state_dict(torch.load(load_path_defended_influence, map_location=device))\n",
        "model_defended_influence.eval() # Set to evaluation mode after loading\n",
        "\n",
        "criterion_defended_influence = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Influence Function Defended model loaded from: {load_path_defended_influence}\")"
      ],
      "metadata": {
        "id": "Or2RFNdQCYuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3739e32"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `model_defended_influence` has been trained, the next step is to evaluate its performance on the target image and compare it with the previous models, similar to how other defense mechanisms were evaluated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "100afee0"
      },
      "source": [
        "model_defended_influence.eval()\n",
        "\n",
        "# Move target image to device and add batch dimension\n",
        "target_image_on_device = target_image.to(device).unsqueeze(0)\n",
        "\n",
        "print(\"\\n--- Re-evaluation of Target Image with Influence-Based Defended Model ---\")\n",
        "print(f\"Original True Class: {train_set.classes[target_true_label]} (Index: {target_true_label})\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_defended_influence = model_defended_influence(target_image_on_device)\n",
        "    probabilities_defended_influence = torch.softmax(outputs_defended_influence, dim=1).squeeze(0) # Remove batch dimension\n",
        "    _, predicted_defended_influence_idx = torch.max(probabilities_defended_influence, 0)\n",
        "\n",
        "predicted_defended_influence_class = train_set.classes[predicted_defended_influence_idx.item()]\n",
        "\n",
        "print(f\"Influence-Based Defended Model's Prediction: {predicted_defended_influence_class} (Index: {predicted_defended_influence_idx.item()})\")\n",
        "\n",
        "# Get top 10 probabilities and classes for the influence-based defended model's prediction\n",
        "top10_probs_defended_influence, top10_indices_defended_influence = torch.topk(probabilities_defended_influence, 10)\n",
        "print(\"Top 10 Predicted Probabilities and Classes (Influence-Based Defended Model):\")\n",
        "for i in range(10):\n",
        "    class_name = train_set.classes[top10_indices_defended_influence[i].item()]\n",
        "    probability = top10_probs_defended_influence[i].item()\n",
        "    print(f\"  {class_name}: {probability:.4f}\")\n",
        "\n",
        "# Explicitly compare 'deer' and 'dog' probabilities across all models\n",
        "print(f\"\\n--- Comparison (Baseline vs. Poisoned vs. Loss-Outlier Defended vs. Adaptive Bilevel Defended vs. Activation Clustering Defended vs. Ensemble Defended vs. Influence-Based Defended) ---\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'deer': {target_probabilities[deer_idx].item():.4f}\")\n",
        "print(f\"Baseline (Unpoisoned Model) Probability for 'dog': {target_probabilities[dog_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'deer': {probabilities_poisoned[deer_idx].item():.4f}\")\n",
        "print(f\"Poisoned Model Probability for 'dog': {probabilities_poisoned[dog_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'deer': {probabilities_defended[deer_idx].item():.4f}\")\n",
        "print(f\"Loss-Outlier Defended Model Probability for 'dog': {probabilities_defended[dog_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'deer': {probabilities_bilevel_defended_adaptive[deer_idx].item():.4f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model Probability for 'dog': {probabilities_bilevel_defended_adaptive[dog_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'deer': {probabilities_defended_activation_clustering[deer_idx].item():.4f}\")\n",
        "print(f\"Activation Clustering Defended Model Probability for 'dog': {probabilities_defended_activation_clustering[dog_idx].item():.4f}\")\n",
        "print(f\"Ensemble Defended Model Probability for 'deer': {ensemble_probabilities_target[deer_idx].item():.4f}\")\n",
        "print(f\"Ensemble Defended Model Probability for 'dog': {ensemble_probabilities_target[dog_idx].item():.4f}\")\n",
        "print(f\"Influence-Based Defended Model Probability for 'deer': {probabilities_defended_influence[deer_idx].item():.4f}\")\n",
        "print(f\"Influence-Based Defended Model Probability for 'dog': {probabilities_defended_influence[dog_idx].item():.4f}\")\n",
        "\n",
        "if predicted_defended_influence_idx.item() == dog_idx:\n",
        "    print(f\"\\nObservation: The influence-based defended model still misclassified the 'deer' image as 'dog'.\")\n",
        "elif predicted_defended_influence_idx.item() == deer_idx:\n",
        "    print(f\"\\nObservation: The influence-based defended model correctly classified the 'deer' image as 'deer'.\")\n",
        "else:\n",
        "    print(f\"\\nObservation: The influence-based defended model predicted the 'deer' image as {predicted_defended_influence_class}.\")\n",
        "\n",
        "# Report overall test accuracy of the influence-based defended model\n",
        "test_loss_defended_influence, test_acc_defended_influence = test(model_defended_influence, test_loader, criterion_defended_influence, device)\n",
        "print(f\"\\nOverall Test Accuracy of Influence-Based Defended Model: {test_acc_defended_influence:.3f}\")\n",
        "\n",
        "# Comparison of overall test accuracies (updated with new model)\n",
        "print(f\"\\n--- Overall Test Accuracy Comparison ---\")\n",
        "print(f\"Baseline Model: {test_acc:.3f}\")\n",
        "print(f\"Poisoned Model: {test_acc_poisoned:.3f}\")\n",
        "print(f\"Loss-Outlier Defended Model: {test_acc_defended:.3f}\")\n",
        "print(f\"Adaptive Bilevel Defended Model: {test_acc_bilevel_defended_adaptive:.3f}\")\n",
        "print(f\"Activation Clustering Defended Model: {test_acc_defended_activation_clustering:.3f}\")\n",
        "print(f\"Ensemble Defended Model: {test_acc_ensemble:.3f}\")\n",
        "print(f\"Influence-Based Defended Model: {test_acc_defended_influence:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd333395"
      },
      "source": [
        "## Compare Results and Visualize\n",
        "\n",
        "### Subtask:\n",
        "Compare the performance of the Influence Function Defended Model against the Baseline, Poisoned, Loss-Outlier Defended, Adaptive Bilevel Defended, and Activation Clustering Defended models. Update the existing visualization cells or create new ones to include these new results. Ensure all plots have appropriate labels and legends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c11b9a"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires updating `prob_df` and `accuracy_df` with the results of the 'Influence Function Defended' model, then regenerating and displaying the bar charts for both probability and accuracy comparisons across all models. I will first update the DataFrames and then generate the plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a6b03ca"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Add a new row to the `prob_df` DataFrame for the \"Influence Function Defended\" model\n",
        "prob_df_updated = prob_df.copy()\n",
        "new_prob_row = pd.DataFrame({\n",
        "    'Model': ['Influence Function Defended'],\n",
        "    'Deer Probability': [probabilities_defended_influence[deer_idx].item()],\n",
        "    'Dog Probability': [probabilities_defended_influence[dog_idx].item()]\n",
        "})\n",
        "prob_df_updated = pd.concat([prob_df_updated, new_prob_row], ignore_index=True)\n",
        "\n",
        "# 2. Add a new row to the `accuracy_df` DataFrame for the \"Influence Function Defended\" model\n",
        "accuracy_df_updated = accuracy_df.copy()\n",
        "new_accuracy_row = pd.DataFrame({\n",
        "    'Model': ['Influence Function Defended'],\n",
        "    'Test Accuracy': [test_acc_defended_influence]\n",
        "})\n",
        "accuracy_df_updated = pd.concat([accuracy_df_updated, new_accuracy_row], ignore_index=True)\n",
        "\n",
        "# 3. Print both the updated `prob_df` and `accuracy_df` DataFrames\n",
        "print(\"\\n--- Updated Probability DataFrame ---\")\n",
        "print(prob_df_updated)\n",
        "print(\"\\n--- Updated Accuracy DataFrame ---\")\n",
        "print(accuracy_df_updated)\n",
        "\n",
        "# 4. Create a bar chart visualizing the 'Deer Probability' and 'Dog Probability' for all models\n",
        "plt.figure(figsize=(16, 8))\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(prob_df_updated['Model']))\n",
        "\n",
        "plt.bar(index, prob_df_updated['Deer Probability'], bar_width, label='Deer Probability', color='skyblue')\n",
        "plt.bar(index + bar_width, prob_df_updated['Dog Probability'], bar_width, label='Dog Probability', color='lightcoral')\n",
        "\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Probability', fontsize=12)\n",
        "plt.title('Deer vs. Dog Probability for Target Image Across All Models', fontsize=14)\n",
        "plt.xticks(index + bar_width / 2, prob_df_updated['Model'], rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Bar chart visualizing 'deer' and 'dog' probabilities across all models displayed.\")\n",
        "\n",
        "# 5. Create a bar chart visualizing the 'Test Accuracy' for all models\n",
        "plt.figure(figsize=(14, 7))\n",
        "bar_width = 0.6\n",
        "index_acc = np.arange(len(accuracy_df_updated['Model']))\n",
        "\n",
        "plt.bar(index_acc, accuracy_df_updated['Test Accuracy'], bar_width, color='dodgerblue')\n",
        "\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Test Accuracy', fontsize=12)\n",
        "plt.title('Overall Test Accuracy Across All Models', fontsize=14)\n",
        "plt.xticks(index_acc, accuracy_df_updated['Model'], rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.ylim(0.8, 1) # Set y-axis limits to focus on relevant accuracy range\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Bar chart visualizing overall test accuracies across all models displayed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519fb1e1"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings for the Influence Function based Data Pruning defense, discussing its effectiveness, impact on overall model accuracy, and comparison to other defense strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b0f32f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **How effective is the Influence Function based Data Pruning defense in mitigating the poisoning attack on the target image?**\n",
        "    The Influence Function based Data Pruning defense was highly effective. After pruning 250 most influential samples, the retrained model successfully corrected the misclassification of the target 'deer' image, classifying it as 'deer' with a probability of 0.7595. The probability for 'dog' (the misclassified class) was significantly reduced to 0.2399.\n",
        "\n",
        "*   **What was the impact of this defense strategy on the overall model accuracy?**\n",
        "    The overall test accuracy of the Influence Function defended model was 0.926. This performance matched the accuracy of the unpoisoned baseline model, indicating that the defense successfully mitigated the attack without degrading general model performance.\n",
        "\n",
        "*   **How does this defense compare to other defense strategies evaluated?**\n",
        "    The Influence Function defense, like Loss-Outlier and Adaptive Bilevel Defenses, successfully corrected the target image's misclassification. However, it achieved this while maintaining an overall test accuracy of 0.926, which is identical to the unpoisoned baseline. This makes it more favorable than Activation Clustering, which was effective on the target but resulted in a notable drop in overall accuracy. While Ensemble methods might achieve higher overall accuracy, they failed to correct the specific target misclassification, highlighting the targeted effectiveness of the Influence Function approach.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The Influence Function defense identified and removed 250 training samples (0.5% of the total training data) that were most influential in causing the target image's misclassification.\n",
        "*   The top 10 most influential samples were all 'dog' images, with the highest influence score being 2022.67, indicating their strong contribution to the target 'deer' image being misclassified as 'dog'.\n",
        "*   The defended model successfully reclassified the target 'deer' image with a 'deer' probability of 0.7595 and a 'dog' probability of 0.2399, effectively reversing the poisoning attack.\n",
        "*   The overall test accuracy of the Influence Function defended model was 0.926, which is on par with the unpoisoned baseline model's accuracy, demonstrating excellent preservation of general model performance.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The Influence Function based data pruning defense offers a balanced approach, effectively mitigating targeted poisoning attacks by identifying and removing problematic samples while preserving the overall model accuracy.\n",
        "*   To further enhance robustness, future work could explore dynamic pruning thresholds or integrate influence function-based pruning as a pre-processing step in a multi-stage defense strategy, potentially combined with other methods for broader attack coverage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a great question! Let's break down how the Influence Function based Data Pruning defense compares to the other methods we've explored:\n",
        "\n",
        "Baseline (Unpoisoned Model):\n",
        "\n",
        "Mechanism: This is our reference model, trained on clean data.\n",
        "Target Image: Correctly classified 'deer' with high confidence, but had a noticeable 'dog' probability (0.6298 'deer' / 0.3664 'dog').\n",
        "Overall Accuracy: Achieved a high overall test accuracy (0.9326).\n",
        "Poisoned Model:\n",
        "\n",
        "Mechanism: Trained on data where 250 'deer' images similar to our target were mislabeled as 'dog'.\n",
        "Target Image: Successfully misclassified the target 'deer' image as 'dog' with very high confidence (0.0733 'deer' / 0.9237 'dog'), demonstrating the attack's success.\n",
        "Overall Accuracy: Maintained a relatively high overall test accuracy (0.9203), showing the stealthiness of the attack.\n",
        "Loss-Outlier Defended Model:\n",
        "\n",
        "Mechanism: Identifies and removes training samples whose loss contributions are significantly higher than the average, based on a statistical outlier threshold.\n",
        "Target Image: Highly effective. It completely corrected the misclassification, classifying the target as 'deer' with extremely high confidence (0.9999 'deer' / 0.0001 'dog').\n",
        "Overall Accuracy: Maintained a good overall test accuracy (0.9168), only slightly lower than the poisoned model.\n",
        "Adaptive Bilevel Defended Model:\n",
        "\n",
        "Mechanism: Dynamically reweights training samples during training. It down-weights samples that contribute to the target's misclassification and up-weights samples that help correct it, adapting the weighting factor over epochs.\n",
        "Target Image: Effective. It corrected the misclassification, classifying the target as 'deer' (0.5627 'deer' / 0.4350 'dog'). While the 'deer' probability wasn't as decisive as the Loss-Outlier defense, it still reversed the attack.\n",
        "Overall Accuracy: Showed a slight decrease in overall test accuracy compared to the poisoned model (0.9087).\n",
        "Activation Clustering Defended Model:\n",
        "\n",
        "Mechanism: Clusters activations from an intermediate layer of the model and removes training samples that fall into a cluster whose dominant label doesn't match the sample's label (i.e., 'outliers' in terms of activation patterns).\n",
        "Target Image: Effective. It correctly classified the target 'deer' image as 'deer' (0.6605 'deer' / 0.0001 'dog'), successfully mitigating the attack.\n",
        "Overall Accuracy: Had a more significant impact on overall test accuracy, dropping to 0.8449.\n",
        "Ensemble Defended Model:\n",
        "\n",
        "Mechanism: Trains multiple models on random subsets of the poisoned training data and aggregates their predictions (e.g., by averaging probabilities) during inference.\n",
        "Target Image: Failed to correct the misclassification. The ensemble still predicted 'dog' for the target 'deer' image, albeit with lower confidence than the single poisoned model (0.2703 'deer' / 0.3727 'dog').\n",
        "Overall Accuracy: Interestingly, this method achieved the highest overall test accuracy (0.9302), even surpassing the baseline. This highlights that high overall accuracy doesn't always imply resilience to targeted attacks.\n",
        "Influence Function based Data Pruning Defended Model:\n",
        "\n",
        "Mechanism: Calculates the 'influence' of each training sample on the target image's misclassification. Samples with high positive influence (meaning they strongly contribute to the misclassification) are identified and removed from the training set.\n",
        "Target Image: Highly effective. It correctly classified the target 'deer' image as 'deer' (0.7595 'deer' / 0.2399 'dog'), successfully reversing the poisoning.\n",
        "Overall Accuracy: Crucially, it achieved this while maintaining an overall test accuracy of 0.9257, which is very close to the baseline and better than most other defense mechanisms that successfully corrected the target image.\n",
        "In Summary:\n",
        "\n",
        "Effectiveness on Target: Loss-Outlier, Adaptive Bilevel, Activation Clustering, and Influence Function defenses all successfully corrected the target image's misclassification. The Ensemble method failed in this specific targeted attack.\n",
        "Impact on Overall Accuracy: The Influence Function defense stands out for successfully mitigating the targeted attack while preserving overall model accuracy very well, matching the baseline model's performance. Loss-Outlier also performed strongly here. Activation Clustering, while fixing the target, caused a notable drop in general accuracy. Adaptive Bilevel had a moderate impact. The Ensemble model had excellent overall accuracy but couldn't fix the targeted misclassification.\n",
        "The Influence Function defense provides a good balance, effectively tackling the specific poisoning attack without significantly compromising the model's general performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "8pRbxzXKBsis"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}